{# Base macros for CLI text rendering across contexts #}
{% macro begin(mode, source_type, source_format, timestamp) -%}
begin={{ mode }} | source={{ source_type }} | format={{ source_format }} | timestamp={{ timestamp }}
{%- endmacro %}

{% macro phase_load(mode, loader_name, latency_ms, char_count) -%}
phase=load | loader={{ loader_name }} | latency_ms={{ latency_ms }} | chars={{ char_count }}
{%- endmacro %}

{% macro phase_extract(mode, url_count, citation_count, total_refs, latency_ms) -%}
phase=extract | urls={{ url_count }} | citations={{ citation_count }} | total_refs={{ total_refs }} | latency_ms={{ latency_ms }}
{%- endmacro %}

{% macro summary(mode, url_count, citation_count, total_refs, latency_ms) -%}
summary={{ mode }} | urls={{ url_count }} | citations={{ citation_count }} | total_refs={{ total_refs }} | latency_ms={{ latency_ms }}
{%- endmacro %}

{% macro end(mode, manifest_id, url_count, citation_count, total_refs, processed_at, latency_ms) -%}
end={{ mode }} | manifest_id={{ manifest_id }} | urls={{ url_count }} | citations={{ citation_count }} | total_refs={{ total_refs }} | processed_at={{ processed_at }} | total_latency_ms={{ latency_ms }}
{%- endmacro %}

{% macro report(mode, url_count, citation_count, total_refs, total_ms, manifest_id, processed_friendly, provider=None, model=None, tokens=None, llm_latency_ms=None, graph_total_ms=None, graph_tokens=None) -%}
üìä Collect Report
‚Ä¢ Mode: {{ mode }}
‚Ä¢ URLs: {{ url_count }} | Citations: {{ citation_count }} | Total: {{ total_refs }}
‚Ä¢ Time: {{ total_ms }} ms
‚Ä¢ Manifest: {{ manifest_id }}
‚Ä¢ Processed: {{ processed_friendly }}
{% if provider and model %}
‚Ä¢ Engine: {{ provider }}/{{ model }}
{% endif %}
{% if tokens %}
‚Ä¢ Tokens: prompt={{ tokens.prompt or 0 }}, completion={{ tokens.completion or 0 }}
{% endif %}
{% if llm_latency_ms is not none %}
‚Ä¢ LLM Latency: {{ llm_latency_ms }} ms
{% endif %}
{% if graph_total_ms is not none %}
‚Ä¢ Graph Total: {{ graph_total_ms }} ms
{% endif %}
{% if graph_tokens %}
‚Ä¢ Graph Tokens: prompt={{ graph_tokens.prompt or 0 }}, completion={{ graph_tokens.completion or 0 }}
{% endif %}
{%- endmacro %}

{% macro llm_run(provider, model, temperature, latency_ms, prompt_tokens, completion_tokens) -%}
ü§ñ {{ provider }}/{{ model }} | üî• {{ temperature }} | ‚è± {{ latency_ms }}ms | üßæ {{ prompt_tokens }}/{{ completion_tokens }}
{%- endmacro %}
